# -*- coding: utf-8 -*-
"""Fix fix _Fraud detection anova c3

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ClbnyH9D0WBm6nMD7eNjS__-VXomAr40
"""

# Import library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_selection import SelectKBest, f_classif
from imblearn.under_sampling import RandomUnderSampler
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import ComplementNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, GridSearchCV
import joblib
import os
import time
import tracemalloc
import numpy as np
import pandas as pd
import sklearn
import sys

from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import SelectKBest, f_classif, chi2
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score
)
from imblearn.under_sampling import RandomUnderSampler

print("===== ENVIRONMENT INFO =====")
print("Python Version      :", sys.version)
print("NumPy Version       :", np.__version__)
print("Pandas Version      :", pd.__version__)
print("Scikit-learn Version:", sklearn.__version__)
print("Joblib Version      :", joblib.__version__)

# Membaca dataset
df = pd.read_csv('/content/Dataset2.csv')

# menampilkan beberapa baris pertama
df.head()

df.info()

df.describe()

df.isnull().sum()

print("=== Nilai Unik Setiap Kolom ===")
for col in df.columns:
    print(f"{col}: {df[col].nunique()} unique values")

print("===  Jumlah Baris Setiap Nilai ===\n")
for col in df.columns:
    print(f"Kolom: {col}")
    print("Jumlah baris setiap nilai:")
    print(df[col].value_counts())
    print("-" * 50)

num_cols = df.select_dtypes(include=['int64','float64']).columns.tolist()

separate_iqr_summary = []

for col in num_cols:

    # --- IQR khusus kelas fraud ---
    fraud_data = df[df['class'] == 1][col]
    Q1_f = fraud_data.quantile(0.25)
    Q3_f = fraud_data.quantile(0.75)
    IQR_f = Q3_f - Q1_f
    lower_f = Q1_f - 1.5 * IQR_f
    upper_f = Q3_f + 1.5 * IQR_f

    fraud_outlier = ((fraud_data < lower_f) | (fraud_data > upper_f)).sum()
    total_fraud = fraud_data.shape[0]

    # --- IQR khusus kelas non-fraud ---
    nonfraud_data = df[df['class'] == 0][col]
    Q1_nf = nonfraud_data.quantile(0.25)
    Q3_nf = nonfraud_data.quantile(0.75)
    IQR_nf = Q3_nf - Q1_nf
    lower_nf = Q1_nf - 1.5 * IQR_nf
    upper_nf = Q3_nf + 1.5 * IQR_nf

    nonfraud_outlier = ((nonfraud_data < lower_nf) | (nonfraud_data > upper_nf)).sum()
    total_nonfraud = nonfraud_data.shape[0]

    separate_iqr_summary.append({
        'Fitur': col,
        'Outlier Fraud (per-IQR fraud)': fraud_outlier,
        'Persentase Fraud (%)': (fraud_outlier / total_fraud * 100) if total_fraud > 0 else 0,
        'Outlier Non-Fraud (per-IQR non-fraud)': nonfraud_outlier,
        'Persentase Non-Fraud (%)': (nonfraud_outlier / total_nonfraud * 100) if total_nonfraud > 0 else 0
    })

separate_outlier_df = pd.DataFrame(separate_iqr_summary)

print("Outlier dengan IQR dihitung terpisah per kelas:")
print(separate_outlier_df)

#Distribusi kelas (Fraud vs Non-Fraud) ---
plt.figure(figsize=(6,4))
df['class'].value_counts().plot(kind='bar')
plt.title("Distribusi Kelas Fraud vs Non-Fraud")
plt.xlabel("Class")
plt.ylabel("Jumlah")
plt.show()

#Histogram fitur numerik ---
df.hist(figsize=(15,10), bins=30)
plt.suptitle("Histogram Fitur Numerik", y=1.02)
plt.show()

#Boxplot untuk mendeteksi outlier ---
num_cols = df.select_dtypes(include=['int64','float64']).columns.tolist()

plt.figure(figsize=(10, 2.5 * len(num_cols)))  # ⬅️ DIPERKECIL

for i, col in enumerate(num_cols, 1):
    plt.subplot(len(num_cols), 1, i)
    sns.boxplot(x=df[col])
    plt.title(f"Boxplot Fitur: {col}", fontsize=10)
    plt.tight_layout()

plt.show()

features_to_plot = ['purchase_value', 'age']

plt.figure(figsize=(12, 5))

for i, feature in enumerate(features_to_plot, 1):
    plt.subplot(1, 2, i)
    sns.boxplot(x='class', y=feature, data=df)
    plt.title(f'Boxplot {feature} berdasarkan Kelas')
    plt.xlabel('Class (0 = Non-Fraud, 1 = Fraud)')
    plt.ylabel(feature)

plt.tight_layout()
plt.show()

plt.figure(figsize=(14,10))
numeric_df = df.select_dtypes(include=['number'])
numeric_corr = numeric_df.corr()
sns.heatmap(
    numeric_corr,
    annot=True,
    fmt=".2f",
    cmap='coolwarm',
    linewidths=0.5,
    square=True
)
plt.title("Correlation Heatmap Fitur Numerik (Dengan Nilai Korelasi)", fontsize=14)
plt.tight_layout()
plt.show()

# FEATURE ENGINEERING


# Convert time columns to datetime objects
df['signup_time'] = pd.to_datetime(df['signup_time'])
df['purchase_time'] = pd.to_datetime(df['purchase_time'])

# --- 1. time_diff (selisih waktu antara signup & purchase dalam detik)
df['time_diff'] = (df['purchase_time'] - df['signup_time']).dt.total_seconds()

# Jika ada nilai negatif atau NaT, ubah jadi 0
df['time_diff'] = df['time_diff'].fillna(0)
df.loc[df['time_diff'] < 0, 'time_diff'] = 0

# --- 2. purchase_hour (jam transaksi)
df['purchase_hour'] = df['purchase_time'].dt.hour

# --- 3. purchase_day (hari transaksi)
df['purchase_day'] = df['purchase_time'].dt.dayofweek   # 0 = Senin

# --- 4. value_group (kategori nilai transaksi)
df['value_group'] = pd.cut(
    df['purchase_value'],
    bins=[0, 20, 40, 80, 200],
    labels=['low', 'medium', 'high', 'very_high']
)

# --- 5. age_group (kategori umur)
df['age_group'] = pd.cut(
    df['age'],
    bins=[0, 25, 40, 60, 100],
    labels=['young', 'adult', 'middle', 'senior']
)

# --- 6. ip_freq (berapa kali IP muncul)
df['ip_freq'] = df.groupby('ip_address')['ip_address'].transform('count')

# --- 7. device_freq (berapa kali device muncul)
df['device_freq'] = df.groupby('device_id')['device_id'].transform('count')

print("=== Feature Engineering Tambahan Selesai ===")
print(df[['time_diff','purchase_hour','purchase_day','value_group','age_group','ip_freq','device_freq']].head())

# Pisahkan target
y = df['class']

X = df.drop('class', axis=1).copy()
# X sementara (seluruh fitur kecuali target)
X_temp = df.drop('class', axis=1)

print("Kolom setelah drop:", X_temp.columns.tolist())
print("Shape X sementara:", X_temp.shape)

numeric_features = df.select_dtypes(include=['int64','float64']).columns.tolist()
numeric_features = [col for col in numeric_features if col != 'class']

numeric_features

from sklearn.feature_selection import SelectKBest, f_classif

selector = SelectKBest(score_func=f_classif, k='all')
selector.fit(df[numeric_features], y)

anova_df = pd.DataFrame({
    'Feature': numeric_features,
    'ANOVA_F_Score': selector.scores_
}).sort_values(by='ANOVA_F_Score', ascending=False)

anova_df

selected_numeric = anova_df[anova_df['ANOVA_F_Score'] > 10]['Feature'].tolist()
selected_numeric

categorical_features = [
    'value_group', 'age_group',
    'browser', 'source', 'sex',

]

categorical_features

from sklearn.feature_selection import SelectKBest, chi2
import pandas as pd

# Salin fitur kategorikal untuk chi-square
X_cat = df[categorical_features].copy()

# One-hot encode dulu (chi-square butuh angka)
X_cat_encoded = pd.get_dummies(X_cat, drop_first=True)

# Chi-Square selection
chi_selector = SelectKBest(score_func=chi2, k='all')
chi_selector.fit(X_cat_encoded, y)

# Buat tabel hasil chi square
chi_scores = pd.DataFrame({
    "Feature": X_cat_encoded.columns,
    "Chi2_Score": chi_selector.scores_
}).sort_values(by="Chi2_Score", ascending=False)

chi_scores

selected_categorical = chi_scores[chi_scores['Chi2_Score'] > 10]['Feature'].tolist()
selected_categorical

# NUMERIC
X_numeric = df[selected_numeric].copy()
# KATEGORICAL
X_cat_full = pd.get_dummies(df[categorical_features], drop_first=True)

# Ambil hanya dummy terpilih
X_categorical_selected = X_cat_full[selected_categorical]

# FINAL FEATURE SELECTION
X_final = pd.concat([X_numeric, X_categorical_selected], axis=1)

print("\n=== SHAPE X_final ===")
print(X_final.shape)

print("\n=== FITUR FINAL ===")
print(list(X_final.columns))

import time
import tracemalloc
import numpy as np
import pandas as pd

from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import MinMaxScaler
from sklearn.naive_bayes import ComplementNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score,
    roc_auc_score
)
from imblearn.under_sampling import RandomUnderSampler

# =====================================================
# FUNGSI FNR & FPR
# =====================================================
def calculate_rates(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(
        y_true, y_pred, labels=[0, 1]
    ).ravel()

    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
    return fnr, fpr

# =====================================================
# STRATIFIED K-FOLD (CV = 3)
# =====================================================
skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)

nb_train_times, nb_pred_times, nb_memories = [], [], []
rf_train_times, rf_pred_times, rf_memories = [], [], []

y_true_all = []
y_pred_nb_all, y_pred_rf_all = [], []
y_pred_proba_nb_all, y_pred_proba_rf_all = [], []

# =====================================================
# CV LOOP
# =====================================================
for train_idx, test_idx in skf.split(X_final, y):

    X_train, X_test = X_final.iloc[train_idx], X_final.iloc[test_idx]
    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

    print(f"Data latih : {X_train.shape[0]} sampel")
    print(f"Data uji   : {X_test.shape[0]} sampel")


    # =================================================
    # RANDOM UNDER SAMPLING (TRAIN ONLY)
    # =================================================
    print("Jumlah kelas sebelum RUS:", y_train.value_counts())
    rus = RandomUnderSampler(random_state=42)
    X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)
    print("Jumlah kelas setelah RUS:", y_train_rus.value_counts())

    # ================== SCALING ==================
    from sklearn.preprocessing import MinMaxScaler

    scaler_nb = MinMaxScaler()
    scaler_rf = MinMaxScaler()

    # NB
    X_train_nb = scaler_nb.fit_transform(X_train_rus)
    X_test_nb  = scaler_nb.transform(X_test)

    # RF
    X_train_rf = scaler_rf.fit_transform(X_train_rus)
    X_test_rf  = scaler_rf.transform(X_test)

    # =================================================
    # COMPLEMENT NAIVE BAYES
    # =================================================

    tracemalloc.start()
    start_train = time.time()

    nb_model = ComplementNB()
    nb_model.fit(X_train_nb, y_train_rus)

    end_train = time.time()
    start_pred = time.time()

    y_pred_nb = nb_model.predict(X_test_nb)
    y_pred_proba_nb = nb_model.predict_proba(X_test_nb)[:, 1]

    end_pred = time.time()
    _, peak_nb = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    nb_train_times.append(end_train - start_train)
    nb_pred_times.append(end_pred - start_pred)
    nb_memories.append(peak_nb / (1024 * 1024))

    # =================================================
    # RANDOM FOREST
    # =================================================
    tracemalloc.start()
    start_train = time.time()

    rf_model = RandomForestClassifier(
        n_estimators=100,
        random_state=42,
        n_jobs=-1
    )
    rf_model.fit(X_train_rus, y_train_rus)

    end_train = time.time()
    start_pred = time.time()

    y_pred_rf = rf_model.predict(X_test)
    y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]

    end_pred = time.time()
    _, peak_rf = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    rf_train_times.append(end_train - start_train)
    rf_pred_times.append(end_pred - start_pred)
    rf_memories.append(peak_rf / (1024 * 1024))

    # =================================================
    # SIMPAN HASIL
    # =================================================
    y_true_all.extend(y_test)
    y_pred_nb_all.extend(y_pred_nb)
    y_pred_rf_all.extend(y_pred_rf)
    y_pred_proba_nb_all.extend(y_pred_proba_nb)
    y_pred_proba_rf_all.extend(y_pred_proba_rf)

# =====================================================
# HASIL AKHIR – COMPLEMENT NAIVE BAYES
# =====================================================
print("\n================ COMPLEMENT NAIVE BAYES (CV=3) ================")
print(classification_report(y_true_all, y_pred_nb_all))
print("Accuracy :", accuracy_score(y_true_all, y_pred_nb_all))
print("Precision:", precision_score(y_true_all, y_pred_nb_all))
print("Recall   :", recall_score(y_true_all, y_pred_nb_all))
print("F1-score :", f1_score(y_true_all, y_pred_nb_all))

fnr_nb, fpr_nb = calculate_rates(y_true_all, y_pred_nb_all)
print("FNR :", fnr_nb)
print("FPR :", fpr_nb)
print("ROC AUC :", roc_auc_score(y_true_all, y_pred_proba_nb_all))
print(f"Total Training Time   : {np.sum(nb_train_times):.4f} detik")
print(f"Total Prediction Time : {np.sum(nb_pred_times):.6f} detik")
print(f"Peak Memory Usage     : {np.max(nb_memories):.2f} MB")

# =====================================================
# HASIL AKHIR – RANDOM FOREST
# =====================================================
print("\n================ RANDOM FOREST (CV=3) ================")
print(classification_report(y_true_all, y_pred_rf_all))
print("Accuracy :", accuracy_score(y_true_all, y_pred_rf_all))
print("Precision:", precision_score(y_true_all, y_pred_rf_all))
print("Recall   :", recall_score(y_true_all, y_pred_rf_all))
print("F1-score :", f1_score(y_true_all, y_pred_rf_all))

fnr_rf, fpr_rf = calculate_rates(y_true_all, y_pred_rf_all)
print("FNR :", fnr_rf)
print("FPR :", fpr_rf)
print("ROC AUC :", roc_auc_score(y_true_all, y_pred_proba_rf_all))
print(f"Total Training Time   : {np.sum(rf_train_times):.4f} detik")
print(f"Total Prediction Time : {np.sum(rf_pred_times):.6f} detik")
print(f"Peak Memory Usage     : {np.max(rf_memories):.2f} MB")

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

# =====================================================
# PLOT ROC CURVE
# =====================================================
fpr_nb, tpr_nb, _ = roc_curve(y_true_all, y_pred_proba_nb_all)
fpr_rf, tpr_rf, _ = roc_curve(y_true_all, y_pred_proba_rf_all)

roc_auc_nb = auc(fpr_nb, tpr_nb)
roc_auc_rf = auc(fpr_rf, tpr_rf)

plt.figure(figsize=(8, 6))
plt.plot(fpr_nb, tpr_nb, color='blue', lw=2, label=f'Complement NB (AUC = {roc_auc_nb:.3f})')
plt.plot(fpr_rf, tpr_rf, color='green', lw=2, label=f'Random Forest (AUC = {roc_auc_rf:.3f})')
plt.plot([0, 1], [0, 1], color='gray', lw=1, linestyle='--')  # garis diagonal random
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve – Complement NB vs Random Forest')
plt.legend(loc='lower right')
plt.grid(alpha=0.3)
plt.show()

from collections import Counter

# =====================================================
# FNR & FPR
# =====================================================
def calculate_rates(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(
        y_true, y_pred, labels=[0, 1]
    ).ravel()

    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
    return fnr, fpr

# =====================================================
# PARAMETER GRID (F1 ORIENTED)
# =====================================================
param_grid_rf = {
    'n_estimators': [200, 300],
    'max_depth': [None, 20],
    'min_samples_split': [5, 10],
    'min_samples_leaf': [2, 5],
    'class_weight': [None, 'balanced']
}

# =====================================================
# STRATIFIED K-FOLD
# =====================================================
skf = StratifiedKFold(
    n_splits=3,
    shuffle=True,
    random_state=42
)

# =====================================================
# INIT STORAGE
# =====================================================
rf_train_times, rf_pred_times, rf_memories = [], [], []
y_true_rf, y_pred_rf, y_pred_proba_rf = [], [], []

best_params_per_fold = []
best_f1_per_fold = []

# =====================================================
# CV LOOP – RANDOM FOREST
# =====================================================
for fold, (train_idx, test_idx) in enumerate(skf.split(X_final, y), start=1):

    print(f"\n========== FOLD {fold} ==========")

    X_train, X_test = X_final.iloc[train_idx], X_final.iloc[test_idx]
    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

    # ================= RANDOM UNDER SAMPLING =================
    rus = RandomUnderSampler(random_state=42)
    X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)

    # ================= GRID SEARCH =================
    tracemalloc.start()
    start_train = time.time()

    rf_base = RandomForestClassifier(
        random_state=42,
        n_jobs=-1
    )

    grid_rf = GridSearchCV(
        estimator=rf_base,
        param_grid=param_grid_rf,
        scoring='f1',
        cv=3,
        n_jobs=-1
    )

    grid_rf.fit(X_train_rus, y_train_rus)
    end_train = time.time()

    # ================= BEST PARAMETER =================
    best_params_per_fold.append(grid_rf.best_params_)
    best_f1_per_fold.append(grid_rf.best_score_)

    print("Best Params :", grid_rf.best_params_)
    print(f"Best CV F1  : {grid_rf.best_score_:.4f}")

    # ================= PREDICTION =================
    start_pred = time.time()

    best_rf = grid_rf.best_estimator_

    y_pred = best_rf.predict(X_test)
    y_prob = best_rf.predict_proba(X_test)[:, 1]

    end_pred = time.time()

    _, peak_mem = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    # ================= STORE =================
    rf_train_times.append(end_train - start_train)
    rf_pred_times.append(end_pred - start_pred)
    rf_memories.append(peak_mem / (1024 * 1024))

    y_true_rf.extend(y_test)
    y_pred_rf.extend(y_pred)
    y_pred_proba_rf.extend(y_prob)

# =====================================================
# BEST PARAMETER SUMMARY
# =====================================================
print("\n================ BEST PARAMETER PER FOLD =================")
for i, (params, score) in enumerate(zip(best_params_per_fold, best_f1_per_fold), start=1):
    print(f"Fold {i}")
    print("Params :", params)
    print(f"CV F1  : {score:.4f}\n")

param_counter = Counter(
    tuple(sorted(p.items())) for p in best_params_per_fold
)

dominant_params, freq = param_counter.most_common(1)[0]

print("=================================================")
print("BEST PARAMETER DOMINAN (PALING SERING MUNCUL)")
print(dict(dominant_params))
print("Muncul sebanyak:", freq, "fold")

# =====================================================
# EVALUASI FINAL RANDOM FOREST
# =====================================================
print("\n================ RANDOM FOREST FINAL =================")
print(classification_report(y_true_rf, y_pred_rf))

print("Accuracy :", accuracy_score(y_true_rf, y_pred_rf))
print("Precision:", precision_score(y_true_rf, y_pred_rf))
print("Recall   :", recall_score(y_true_rf, y_pred_rf))
print("F1-score :", f1_score(y_true_rf, y_pred_rf))

fnr, fpr = calculate_rates(y_true_rf, y_pred_rf)
print("FNR :", fnr)
print("FPR :", fpr)

# ================= ROC AUC =================
roc_auc_rf = roc_auc_score(y_true_rf, y_pred_proba_rf)
print("ROC AUC :", roc_auc_rf)

# ================= RESOURCE USAGE =================
print(f"TOTAL Training Time   : {np.sum(rf_train_times):.4f} detik")
print(f"TOTAL Prediction Time : {np.sum(rf_pred_times):.6f} detik")
print(f"PEAK Memory Usage     : {np.max(rf_memories):.2f} MB")

# =====================================================
# FNR & FPR FUNCTION
# =====================================================
def calculate_rates(y_true, y_pred):
    tn, fp, fn, tp = confusion_matrix(
        y_true, y_pred, labels=[0, 1]
    ).ravel()
    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0
    return fnr, fpr

# =====================================================
# PARAMETER GRID CNB
# =====================================================
param_grid_nb = {
    'alpha': [0.1, 0.3, 0.5, 1.0, 1.5, 2.0],
    'norm': [True, False]
}

# =====================================================
# STRATIFIED K-FOLD
# =====================================================
skf = StratifiedKFold(
    n_splits=3,
    shuffle=True,
    random_state=42
)

# =====================================================
# INIT STORAGE
# =====================================================
nb_train_times, nb_pred_times, nb_memories = [], [], []
y_true_nb, y_pred_nb, y_pred_proba_nb = [], [], []

# SIMPAN BEST PARAM
best_params_nb = []
best_f1_nb = []

# =====================================================
# CV LOOP – COMPLEMENT NAIVE BAYES
# =====================================================
for fold, (train_idx, test_idx) in enumerate(skf.split(X_final, y), start=1):

    print(f"\n========== FOLD {fold} ==========")

    X_train, X_test = X_final.iloc[train_idx], X_final.iloc[test_idx]
    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]

    # ================= RUS =================
    rus = RandomUnderSampler(random_state=42)
    X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)

    # ================= SCALING =================
    scaler = MinMaxScaler()
    X_train_nb = scaler.fit_transform(X_train_rus)
    X_test_nb = scaler.transform(X_test)

    # ================= GRID SEARCH =================
    tracemalloc.start()
    start_train = time.time()

    nb_base = ComplementNB()
    grid_nb = GridSearchCV(
        estimator=nb_base,
        param_grid=param_grid_nb,
        scoring='f1',
        cv=3,
        n_jobs=-1
    )

    grid_nb.fit(X_train_nb, y_train_rus)
    end_train = time.time()

    # ================= SIMPAN BEST PARAMETER =================
    best_params_nb.append(grid_nb.best_params_)
    best_f1_nb.append(grid_nb.best_score_)

    print("Best Params :", grid_nb.best_params_)
    print(f"Best CV F1  : {grid_nb.best_score_:.4f}")

    # ================= PREDIKSI =================
    start_pred = time.time()

    best_nb = grid_nb.best_estimator_

    y_pred = best_nb.predict(X_test_nb)
    y_prob = best_nb.predict_proba(X_test_nb)[:, 1]

    end_pred = time.time()

    _, peak_mem = tracemalloc.get_traced_memory()
    tracemalloc.stop()

    # ================= STORE =================
    nb_train_times.append(end_train - start_train)
    nb_pred_times.append(end_pred - start_pred)
    nb_memories.append(peak_mem / (1024 * 1024))

    y_true_nb.extend(y_test)
    y_pred_nb.extend(y_pred)
    y_pred_proba_nb.extend(y_prob)

# =====================================================
# BEST PARAMETER PER FOLD
# =====================================================
print("\n================ BEST PARAMETER PER FOLD =================")
for i, (params, score) in enumerate(zip(best_params_nb, best_f1_nb), start=1):
    print(f"Fold {i}")
    print("Params :", params)
    print(f"CV F1  : {score:.4f}\n")

# =====================================================
# PARAMETER PALING DOMINAN
# =====================================================
param_counter_nb = Counter(
    tuple(sorted(p.items())) for p in best_params_nb
)

dominant_params_nb, freq_nb = param_counter_nb.most_common(1)[0]

print("=================================================")
print("BEST PARAMETER DOMINAN COMPLEMENT NAIVE BAYES")
print(dict(dominant_params_nb))
print("Muncul sebanyak:", freq_nb, "fold")

# =====================================================
# EVALUASI AKHIR
# =====================================================
print("\n================ COMPLEMENT NAIVE BAYES FINAL =================")
print(classification_report(y_true_nb, y_pred_nb))

print("Accuracy :", accuracy_score(y_true_nb, y_pred_nb))
print("Precision:", precision_score(y_true_nb, y_pred_nb))
print("Recall   :", recall_score(y_true_nb, y_pred_nb))
print("F1-score :", f1_score(y_true_nb, y_pred_nb))

fnr, fpr = calculate_rates(y_true_nb, y_pred_nb)
print("FNR :", fnr)
print("FPR :", fpr)

# ================= ROC AUC =================
roc_auc_nb = roc_auc_score(y_true_nb, y_pred_proba_nb)
print("ROC AUC :", roc_auc_nb)

# ================= RESOURCE SUMMARY =================
print(f"TOTAL Training Time   : {np.sum(nb_train_times):.4f} detik")
print(f"TOTAL Prediction Time : {np.sum(nb_pred_times):.6f} detik")
print(f"PEAK Memory Usage     : {np.max(nb_memories):.2f} MB")

# ================= ROC CURVE GABUNGAN =================
plt.figure(figsize=(8, 6))

# Random Forest (BLUE)
plt.plot(
    fpr_rf,
    tpr_rf,
    color="blue",
    lw=2,
    label=f'Random Forest (AUC = {roc_auc_rf:.3f})'
)

# Complement Naive Bayes (GREEN)
plt.plot(
    fpr_nb,
    tpr_nb,
    color="green",
    lw=2,
    label=f'Complement NB (AUC = {roc_auc_nb:.3f})'
)

# Diagonal baseline (GRAY)
plt.plot(
    [0, 1],
    [0, 1],
    linestyle='--',
    color="gray",
    lw=1.5,
    label="Baseline (Random Guess)"
)

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison – Random Forest vs Complement Naive Bayes')
plt.legend(loc='lower right')
plt.grid(alpha=0.3)
plt.show()

import joblib
import os

os.makedirs("models", exist_ok=True)

file_path = "models/rf_baseline_cv3.pkl"

joblib.dump(
    {
        "scaler": scaler_rf,
        "feature_selector": None,
        "model": rf_model,
        "features": list(X_final.columns),
        "model_type": "RF Baseline (no refit)"
    },
    file_path
)

size_mb = os.path.getsize(file_path) / (1024 * 1024)
print(" RF Baseline disimpan")
print(f" Size: {size_mb:.2f} MB")

file_path = "models/rf_tuned_cv3.pkl"

joblib.dump(
    {
        "scaler": scaler_rf,
        "feature_selector": None,
        "model": best_rf,
        "features": list(X_final.columns),
        "model_type": "RF Tuned (CV, no refit)"
    },
    file_path
)

size_mb = os.path.getsize(file_path) / (1024 * 1024)
print(" RF Tuned disimpan")
print(f" Size: {size_mb:.2f} MB")

file_path = "models/cnb_baseline_cv3.pkl"

joblib.dump(
    {
        "scaler": scaler_nb,
        "model": nb_model,
        "features": list(X_final.columns),
        "model_type": "CNB Baseline (no refit)"
    },
    file_path
)

size_mb = os.path.getsize(file_path) / (1024 * 1024)
print(" CNB Baseline disimpan")
print(f" Size: {size_mb:.4f} MB")

file_path = "models/cnb_tuned_cv3.pkl"

joblib.dump(
    {
        "scaler": scaler,
        "model": best_nb,
        "features": list(X_final.columns),
        "model_type": "CNB Tuned (CV, no refit)"
    },
    file_path
)

size_mb = os.path.getsize(file_path) / (1024 * 1024)
print(" CNB Tuned disimpan")
print(f" Size: {size_mb:.4f} MB")